apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: netflix-ml-prod-
  namespace: argo
spec:
  entrypoint: ml-pipeline
  serviceAccountName: default
  volumeClaimTemplates:
  - metadata: { name: workdir }
    spec:
      accessModes: [ "ReadWriteOnce" ]
      resources: { requests: { storage: 1Gi } }

  templates:
  - name: ml-pipeline
    dag:
      tasks:
      - name: setup-feature-store
        template: setup-fs
      - name: fetch-features
        template: fetch-features
        dependencies: [setup-feature-store]
      - name: train-model
        template: train-model
        dependencies: [fetch-features]
      - name: evaluate-and-register
        template: evaluate-and-register
        dependencies: [train-model]

  # New Step: Initialize a simple SQLite feature store
  - name: setup-fs
    container:
      image: python:3.9
      command: [python, -c]
      args:
        - |
          import sqlite3, pandas as pd
          print("Initializing Feature Store (SQLite)...")
          conn = sqlite3.connect('/work/feature_store.db')
          # Create a dummy feature table
          features_df = pd.DataFrame({
              'user_id': [f'user_{i}' for i in range(100)],
              'avg_rating': [round(3.5 + i/100, 2) for i in range(100)],
              'movies_watched': [10 + i for i in range(100)]
          })
          features_df.to_sql('user_features', conn, if_exists='replace', index=False)
          conn.close()
          print("✓ Feature Store initialized.")
      volumeMounts:
      - name: workdir
        mountPath: /work

  # Updated Step: Fetch features from our store
  - name: fetch-features
    container:
      image: python:3.9
      command: [python, -c]
      args:
        - |
          import sqlite3, pandas as pd
          print("Fetching features from Feature Store...")
          conn = sqlite3.connect('/work/feature_store.db')
          df = pd.read_sql_query("SELECT * FROM user_features LIMIT 10", conn)
          print("Sample features:")
          print(df.head())
          df.to_csv('/work/training_data.csv', index=False)
          conn.close()
          print("✓ Features fetched and saved.")
      volumeMounts:
      - name: workdir
        mountPath: /work

  # Updated Step: Train and log to the in-cluster MLflow
  - name: train-model
    container:
      image: continuumio/miniconda3 # Has numpy/sklearn pre-installed
      command: [/bin/bash, -c]
      args:
        - |
          pip install mlflow==2.8.0 pandas
          python -c "
          import mlflow, pickle, pandas as pd
          from sklearn.ensemble import RandomForestRegressor
          
          print('Training model...')
          # Use the MLflow service name inside the cluster
          mlflow.set_tracking_uri('http://mlflow-service.argo.svc.cluster.local:5000')
          mlflow.set_experiment('Netflix Retraining Pipeline')

          with mlflow.start_run() as run:
              df = pd.read_csv('/work/training_data.csv')
              X = df[['movies_watched']]
              y = df['avg_rating']
              
              n_estimators = 10
              mlflow.log_param('n_estimators', n_estimators)
              
              model = RandomForestRegressor(n_estimators=n_estimators, random_state=42)
              model.fit(X, y)
              
              # Log model to MLflow
              mlflow.sklearn.log_model(model, 'model')
              print(f'Model trained. Run ID: {run.info.run_id}')
              
              # Pass run_id to the next step
              with open('/work/run_id.txt', 'w') as f:
                  f.write(run.info.run_id)
          "
      volumeMounts:
      - name: workdir
        mountPath: /work

  # Updated Step: Evaluate and register the model
  - name: evaluate-and-register
    container:
      image: continuumio/miniconda3
      command: [/bin/bash, -c]
      args:
        - |
          pip install mlflow==2.8.0 pandas
          python -c "
          import mlflow, json
          from sklearn.metrics import mean_squared_error
          
          print('Evaluating and registering model...')
          mlflow.set_tracking_uri('http://mlflow-service.argo.svc.cluster.local:5000')
          
          with open('/work/run_id.txt', 'r') as f:
              run_id = f.read()
          
          # Load model from MLflow
          logged_model_uri = f'runs:/{run_id}/model'
          model = mlflow.pyfunc.load_model(logged_model_uri)
          
          # Evaluate (dummy evaluation)
          mse = 0.123
          mlflow.log_metric('mse', mse, run_id=run_id)
          print(f'MSE: {mse}')

          if mse < 0.15:
              print('Model performance is good. Registering model...')
              # Register model in MLflow Model Registry
              mlflow.register_model(logged_model_uri, 'netflix-recommender')
              print('✓ Model registered successfully!')
          else:
              print('✗ Model performance is below threshold. Not registering.')
          "
      volumeMounts:
      - name: workdir
        mountPath: /work